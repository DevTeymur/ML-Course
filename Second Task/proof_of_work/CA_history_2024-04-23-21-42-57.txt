2024-04-23-21-42-57 392 1 from cvxopt import matrix	from cvxopt import solvers		def make_H_k(X,t, kernel_function, param):	    n_samples = len(X)	    H = np.zeros((n_samples, n_samples))	    for i in range(n_samples):	        for j in range(n_samples):	            H[i, j] = t[i] * t[j] * kernel_function(X[i], X[j], param)	    return H	    	def setup_optimization_soft_k(X,t,C, kernel_function, param):	    n_samples = len(X)	    H = make_H_k(X, t, kernel_function, param)	    P = matrix(H, tc='d')	    q = matrix(-1.0, (n_samples, 1))	    G = matrix(np.diag(-1.0 * np.ones(n_samples)), tc='d')	    h = matrix(np.zeros(n_samples))	    A = matrix(t.reshape(1, -1), tc='d')	    b = matrix(0.0)	    return P, q, G, h, A, b	    	def compute_support_vectors(X,t,alphas, tolerance=1e-5):	    sv_idx = alphas > tolerance	    return X[sv_idx], t[sv_idx], alphas[sv_idx], sv_idx	    	def train_slmkc(X, t, C, kernel_function, param):	    n_samples = len(X)	    P, q, G, h, A, b = setup_optimization_soft_k(X, t, C, kernel_function, param)	    sol = solvers.qp(P, q, G, h, A, b)	    alphas = np.array(sol['x']).flatten()	    return compute_support_vectors(X, t, alphas)	    	def compute_bias(kernel_function, param, C, model):	    support_vectors, support_targets, support_alphas, _ = model	    bias = np.mean(support_targets - np.sum(support_alphas * support_targets * kernel_function(support_vectors, support_vectors, param), axis=0))	    return bias	    	def score_slmkc(X_test, kernel_function, param, C, model):	    support_vectors, support_targets, support_alphas, _ = model	    return np.sum(support_alphas * support_targets * kernel_function(support_vectors, X_test, param), axis=0) + compute_bias(kernel_function, param, C, model)	    	def test_slmkc(X_test, kernel_function, param, C, model):	    scores = score_slmkc(X_test, kernel_function, param, C, model)	    return np.sign(scores)	    	class SoftLargeMarginKernelClassifier(object):	    def __init__(self, C, kernel_function, param):	            self.C = C	            self.kernel_function = kernel_function	            self.param = param	            self.support_vectors = None	            self.support_targets = None	            self.support_alphas = None	            self.support_ids = None		    def fit(self, X, y):	        self.support_vectors, self.support_targets, self.support_alphas, self.support_ids = train_slmkc(X, y, self.C, self.kernel_function, self.param)		    def predict(self, X):	        return test_slmkc(X, self.kernel_function, self.param, self.C, (self.support_vectors, self.support_targets, self.support_alphas, self.support_ids))		    def decision_function(self, X):	        return score_slmkc(X, self.kernel_function, self.param, self.C, (self.support_vectors, self.support_targets, self.support_alphas, self.support_ids))		    	save_history()
2024-04-23-21-42-57 392 2 get_ipython().run_line_magic('matplotlib', 'inline')	import matplotlib.pyplot as plt	from mpl_toolkits.mplot3d import Axes3D	import numpy as np	import scipy as sp	import cvxopt	from submission_utils import save_history, check_and_prepare_for_submission	from warnings import simplefilter	# ignore all future warnings	simplefilter(action='ignore', category=FutureWarning)
2024-04-23-21-42-57 392 3 import random	    	def permute(seq):	    # Converting to list, permuting, and converting back to string	    seq_array = np.array(list(seq))	    return ''.join(np.random.permutation(seq_array))		def generate_master_sequence(alphabet_size, seq_length, start_char=70):	    return ''.join(chr(np.random.randint(start_char, start_char + alphabet_size)) for _ in range(seq_length))		def perturb(master_sequence, noise):	    seq_array = np.array(list(master_sequence))	    num_perturbations = int(len(seq_array) * noise)	    # Depending on the noise level swaping the places of random characters	    for _ in range(num_perturbations):	        i1, i2 = np.random.choice(len(seq_array), 2, replace=False)	        seq_array[i1], seq_array[i2] = seq_array[i2], seq_array[i1]	    return ''.join(seq_array)		def randomize_length(seqs, endpoint_trim_dim):	    randomized_seqs = []	    for seq in seqs:	        trim_length = np.random.randint(0, endpoint_trim_dim + 1)	        randomized_seqs.append(seq[trim_length:-trim_length])	    return randomized_seqs		def make_single_cluster_data(master_sequence, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    inliners = [perturb(master_sequence, inliner_noise) for _ in range(n_inliners)]	    outliers = [perturb(permute(master_sequence), outlier_noise) for _ in range(n_outliers)]		    all_seqs = inliners + outliers 	        	    if endpoint_trim_dim is not None:	        all_seqs = randomize_length(all_seqs, endpoint_trim_dim)	    return all_seqs		def make_data(master_sequence, n_clusters, cluster_centres_noise, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    sequences, targets = [], []	    for i in range(n_clusters):	        cluster_centre = perturb(master_sequence, cluster_centres_noise)	        cluster_seqs = make_single_cluster_data(cluster_centre, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim)	        sequences.extend(cluster_seqs)	        targets.extend([i] * len(cluster_seqs))	    return sequences, targets	    	save_history()
2024-04-23-21-42-57 392 4 # DELETE AT THE END		np.random.seed(42)		# For example 1	seq = "temperature"	permuted_seq = permute(seq)	print(f'Original sequence: {seq}, permuted sequence: {permuted_seq}')		# For example 2	master_seq = generate_master_sequence(alphabet_size=5, seq_length=10)	print("Master sequence:", master_seq)		# For example 3	perturbed_seq = perturb(master_seq, noise=0.8)	print("Perturbed sequence:", perturbed_seq)		# For example 4	seqs = ["salam", "temperature", "verynice"]	randomized_seqs = randomize_length(seqs, endpoint_trim_dim=2)	print("Randomized sequences:", randomized_seqs)		# For example 5	master_sequence = "againtemperature"	inlier_seqs = make_single_cluster_data(master_sequence, n_inliners=3, n_outliers=2, inliner_noise=0.1, outlier_noise=0.5)	print("Inlier sequences:", inlier_seqs[:3])	print("Inlier sequences:", inlier_seqs[3:])		# For example 6	master_sequence = "anotherwordthatdoesnotexist"	seqs, targets = make_data(master_sequence, n_clusters=2, cluster_centres_noise=0.1, n_inliners=3, n_outliers=2, inliner_noise=0.1, outlier_noise=0.5)	print("Generated sequences:", seqs)	print("Cluster labels:", targets)
2024-04-23-21-42-57 392 5 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 6 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 7 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 8 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 9 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 10 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 11 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 12 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 13 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 14 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 15 from sklearn.tree import DecisionTreeClassifier		def train_ab(X_train, y_train, param):	    n_instances = len(X_train)	    weights = np.ones(n_instances) / n_instances	    models, alphas= [], []	    for _ in range(100): 	        model = DecisionTreeClassifier(max_depth=param)	        model.fit(X_train, y_train, sample_weight=weights)	        y_pred = model.predict(X_train)	        	        errors = weights[y_pred != y_train].sum()	        alpha = 0.5 * np.log((1 - errors) / errors)	        	        weights *= np.exp(-alpha * y_train * y_pred)	        weights /= np.sum(weights)	    	        models.append(model)	        alphas.append(alpha)	    	    return list(zip(models, alphas))		def test_ab(X_test, models):	    preds = np.zeros(len(X_test))	    for model, alpha in models:	        preds += alpha * model.predict(X_test)	    return np.sign(preds).astype(int)	    	class AdaBoostClassifier():	    def __init__(self, max_depth=1):	        self.max_depth = max_depth	        self.models = None	    	    def fit(self, X, y):	        self.models = train_ab(X, y, self.max_depth)		    def predict(self, X):	        return test_ab(X, self.models)		def make_bootstrap(data_matrix, targets):	    n_instances = len(data_matrix)	    bootstrap_sample_ids = np.random.choice(n_instances, size=n_instances, replace=True)	    bootstrap_data_maxtrix = data_matrix[bootstrap_sample_ids]	    bootstrap_targets = targets[bootstrap_sample_ids]		    oob_sample_ids = np.setdiff1d(np.arange(n_instances), bootstrap_sample_ids) #	    oob_data_matrix = data_matrix[oob_sample_ids]	    oob_targets = targets[oob_sample_ids]		    return bootstrap_data_maxtrix, bootstrap_targets, bootstrap_sample_ids, oob_data_matrix, oob_targets, oob_sample_ids		def train_rfc(X_train, y_train, param):	    models = []	    for _ in range(100):  	        bootstrap_data_matrix, bootstrap_targets, _, _, _, _ = make_bootstrap(X_train, y_train)	        model = DecisionTreeClassifier(max_depth=param)	        model.fit(bootstrap_data_matrix, bootstrap_targets)	        models.append(model)	    return models		def test_rfc(X_test, models):	    predictions = np.zeros(len(X_test))	    for model in models:	        predictions += model.predict(X_test)	    return np.sign(predictions).astype(int)		class RandomForestClassifier:	    def __init__(self, max_depth=1):	        self.max_depth = max_depth	        self.models = []	    	    def fit(self, X, y):	        self.models = train_rfc(X, y, self.max_depth)	    	    def predict(self, X):	        return test_rfc(X, self.models)		save_history()
2024-04-23-21-42-57 392 16 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 17 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 18 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 19 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 20 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 21 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 22 import copy 		def train_OvO(X_train, y_train, estimator):	    unique_classes = np.unique(y_train)	    estimators = {}	    	    for i in range(len(unique_classes)):	        for j in range(i + 1, len(unique_classes)):	            class1 = unique_classes[i]	            class2 = unique_classes[j]	            	            X_pair, y_pair = [], []	            for X_sample, y_sample in zip(X_train, y_train):	                if y_sample == class1 or y_sample == class2:	                    X_pair.append(X_sample)	                    y_pair.append(y_sample)	            	            model = copy.deepcopy(estimator)	            model.fit(X_pair, y_pair)	            estimators[(class1, class2)] = model	    	    return estimators	    	    	def test_OvO(X_test, estimators):	    preds = []	    for sample in X_test:	        class_votes = {}	        for pair, model in estimators.items():	            pred = model.predict([sample])[0]	            if pred in class_votes:	                class_votes[pred] += 1	            else:	                class_votes[pred] = 1	        	        majority_class = max(class_votes, key=class_votes.get)	        preds.append(majority_class)	    	    return np.array(preds)	    	    	class OVOClassifier():	    def __init__(self, estimator):	        self.estimator = estimator	        self.estimators = {}	    	    def fit(self, X, y):	        self.estimators = train_OvO(X, y, self.estimator)	    	    def predict(self, X):	        return test_OvO(X, self.estimators)	    	save_history()
2024-04-23-21-42-57 392 23 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 24 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 25 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 26 from sklearn.model_selection import train_test_split		def compute_learning_curve(estimator, X, y, test_size, n_steps, n_repetitions):	    train_sizes = np.linspace(0.1, 1.0, n_steps)	    train_errors = np.zeros((n_repetitions, n_steps))	    test_errors = np.zeros((n_repetitions, n_steps))	    	    for i in range(n_repetitions):	        for j, size in enumerate(train_sizes):	            X_train, _, y_train, _ = train_test_split(X, y, test_size=size, random_state=i)	            X_test, _, y_test, _ = train_test_split(X, y, test_size=test_size, random_state=i)	            	            estimator.fit(X_train, y_train)	            train_pred = estimator.predict(X_train)	            test_pred = estimator.predict(X_test)	            	            train_errors[i, j] = np.mean(train_pred != y_train)	            test_errors[i, j] = np.mean(test_pred != y_test)	    	    train_errors_mean = np.mean(train_errors, axis=0)	    test_errors_mean = np.mean(test_errors, axis=0)	    	    return train_sizes * X.shape[0], train_errors_mean, test_errors_mean	    	    	def plot_learning_curve(sizes, train_errors, test_errors):	    plt.figure(figsize=(8, 6))	    plt.plot(sizes, train_errors, color='r', label='train')	    plt.plot(sizes, test_errors, color='b', label='test')	    plt.legend(loc='best')	    plt.grid(True)	    plt.show()	    	save_history()
2024-04-23-21-42-57 392 27 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 28 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 29 def histogram_kernel(seq_1, seq_2, param=None):	    hist_1 = {char: seq_1.count(char) for char in set(seq_1)}	    hist_2 = {char: seq_2.count(char) for char in set(seq_2)}	    	    intersection = sum(min(hist_1.get(char, 0), hist_2.get(char, 0)) for char in set(seq_1) | set(seq_2))	    union = sum(max(hist_1.get(char, 0), hist_2.get(char, 0)) for char in set(seq_1) | set(seq_2))		    return intersection / union if union != 0 else 0			def kmer_kernel(seq_1, seq_2, param=2):	    k = param	    kmers_1 = [seq_1[i:i+k] for i in range(len(seq_1) - k + 1)]	    kmers_2 = [seq_2[i:i+k] for i in range(len(seq_2) - k + 1)]	    	    intersection = len(set(kmers_1) & set(kmers_2))	    union = len(set(kmers_1) | set(kmers_2))	    	    return intersection / union if union != 0 else 0		def get_gram_matrix(seqs, kernel_func, param):	    n = len(seqs)	    gram_matrix = np.zeros((n, n))	    for i in range(n):	        for j in range(i, n):	            gram_matrix[i, j] = kernel_func(seqs[i], seqs[j], param)	            gram_matrix[j, i] = gram_matrix[i, j]  # Symmetric matrix	    return gram_matrix		def gram_matrix_to_distance_matrix(G):	    n = len(G)	    D = np.zeros((n, n))	    for i in range(n):	        for j in range(n):	            D[i, j] = np.sqrt(G[i, i] + G[j, j] - 2 * G[i, j])	    return D			from sklearn.manifold import MDS		def mds_plot(seqs, y=None, kernel_func=None, param=None):	    if kernel_func is None:	        kernel_func = histogram_kernel  # Default to histogram kernel	    	    gram_matrix = get_gram_matrix(seqs, kernel_func, param)	    distance_matrix = gram_matrix_to_distance_matrix(gram_matrix)	    	    mds = MDS(n_components=2, dissimilarity='precomputed')	    X_mds = mds.fit_transform(distance_matrix)	    	    plt.figure(figsize=(8, 6))	    colors = ['r', 'b']	    if y is None:	        plt.scatter(X_mds[:, 0], X_mds[:, 1], s=50, alpha=0.8)	    else:	        labels = np.unique(y)	        for label in labels:	            plt.scatter(X_mds[y == label, 0], X_mds[y == label, 1], s=50, alpha=0.8, c=colors[label])	    plt.show()	    	save_history()
2024-04-23-21-42-57 392 30 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 31 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 32 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 33 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 34 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-04-23-21-42-57 392 35 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		mds_plot(seqs, y=targets, kernel_func=histogram_kernel, param=None)		mds_plot(seqs, y=targets, kernel_func=kmer_kernel, param=3)
2024-04-23-21-42-57 392 36 from cvxopt import matrix	from cvxopt import solvers		def make_H_k(X,t, kernel_function, param):	    n_samples = len(X)	    H = np.zeros((n_samples, n_samples))	    for i in range(n_samples):	        for j in range(n_samples):	            H[i, j] = t[i] * t[j] * kernel_function(X[i], X[j], param)	    return H	    	def setup_optimization_soft_k(X,t,C, kernel_function, param):	    n_samples = len(X)	    H = make_H_k(X, t, kernel_function, param)	    P = matrix(H, tc='d')	    q = matrix(-1.0, (n_samples, 1))	    G = matrix(np.diag(-1.0 * np.ones(n_samples)), tc='d')	    h = matrix(np.zeros(n_samples))	    A = matrix(t.reshape(1, -1), tc='d')	    b = matrix(0.0)	    return P, q, G, h, A, b	    	def compute_support_vectors(X,t,alphas, tolerance=1e-5):	    sv_idx = alphas > tolerance	    return X[sv_idx], t[sv_idx], alphas[sv_idx], sv_idx	    	def train_slmkc(X, t, C, kernel_function, param):	    n_samples = len(X)	    P, q, G, h, A, b = setup_optimization_soft_k(X, t, C, kernel_function, param)	    sol = solvers.qp(P, q, G, h, A, b)	    alphas = np.array(sol['x']).flatten()	    return compute_support_vectors(X, t, alphas)	    	def compute_bias(kernel_function, param, C, model):	    support_vectors, support_targets, support_alphas, _ = model	    bias = np.mean(support_targets - np.sum(support_alphas * support_targets * kernel_function(support_vectors, support_vectors, param), axis=0))	    return bias	    	def score_slmkc(X_test, kernel_function, param, C, model):	    support_vectors, support_targets, support_alphas, _ = model	    return np.sum(support_alphas * support_targets * kernel_function(support_vectors, X_test, param), axis=0) + compute_bias(kernel_function, param, C, model)	    	def test_slmkc(X_test, kernel_function, param, C, model):	    scores = score_slmkc(X_test, kernel_function, param, C, model)	    return np.sign(scores)	    	class SoftLargeMarginKernelClassifier(object):	    def __init__(self, C, kernel_function, param):	            self.C = C	            self.kernel_function = kernel_function	            self.param = param	            self.support_vectors = None	            self.support_targets = None	            self.support_alphas = None	            self.support_ids = None		    def fit(self, X, y):	        self.support_vectors, self.support_targets, self.support_alphas, self.support_ids = train_slmkc(X, y, self.C, self.kernel_function, self.param)		    def predict(self, X):	        return test_slmkc(X, self.kernel_function, self.param, self.C, (self.support_vectors, self.support_targets, self.support_alphas, self.support_ids))		    def decision_function(self, X):	        return score_slmkc(X, self.kernel_function, self.param, self.C, (self.support_vectors, self.support_targets, self.support_alphas, self.support_ids))		    	save_history()
2024-04-23-21-42-57 392 37 from sklearn.manifold import MDS		def plot_seq_2d(seqs, y=None, preds=None, is_support=None, kernel_func=None, param=None):	    # Gram matrix computation	    gram_matrix = get_gram_matrix(seqs, kernel_func, param)		    # Distance matrix computation	    distance_matrix = gram_matrix_to_distance_matrix(gram_matrix)		    # MDS part	    mds = MDS(n_components=2, dissimilarity='precomputed')	    embedded_coords = mds.fit_transform(distance_matrix)		    # Plotting part	    plt.figure(figsize=(8, 6))	    if y is not None:	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(embedded_coords[y == label, 0], embedded_coords[y == label, 1], s=50, alpha=0.8, label=label)	    else:	        plt.scatter(embedded_coords[:, 0], embedded_coords[:, 1], s=50, alpha=0.8)		    # Highlighting the missclassified points	    if preds is not None and y is not None:	        missclassified = y != preds	        plt.scatter(embedded_coords[missclassified, 0], embedded_coords[missclassified, 1], s=50, marker='x', color='red', label='missclassified')		    # Highlighting the support vectors	    if is_support is not None:	        coords_support = embedded_coords[is_support]	        plt.scatter(coords_support[:, 0], coords_support[:, 1], s=50, facecolors='none', edgecolors='black', label='support vectors')		    n_instances = len(seqs)	    n_mistakes = len(missclassified[missclassified])	    mistake_percentage = (n_mistakes / n_instances) * 100	    n_support_vectors = len(coords_support)	    support_vector_percentage = (n_support_vectors / n_instances) * 100		    title = f"Number of Instances: {n_instances}, Mistake Count: {n_mistakes} ({mistake_percentage:.2f}%)" \	        f"\nSupport Vector Count: {n_support_vectors} ({support_vector_percentage:.2f}%)"	    plt.title(title)	    plt.show()		save_history()
2024-04-23-21-42-57 392 38 import random	    	def permute(seq):	    # Converting to list, permuting, and converting back to string	    seq_array = np.array(list(seq))	    return ''.join(np.random.permutation(seq_array))		def generate_master_sequence(alphabet_size, seq_length, start_char=70):	    return ''.join(chr(np.random.randint(start_char, start_char + alphabet_size)) for _ in range(seq_length))		def perturb(master_sequence, noise):	    seq_array = np.array(list(master_sequence))	    num_perturbations = int(len(seq_array) * noise)	    # Depending on the noise level swaping the places of random characters	    for _ in range(num_perturbations):	        i1, i2 = np.random.choice(len(seq_array), 2, replace=False)	        seq_array[i1], seq_array[i2] = seq_array[i2], seq_array[i1]	    return ''.join(seq_array)		def randomize_length(seqs, endpoint_trim_dim):	    randomized_seqs = []	    for seq in seqs:	        trim_length = np.random.randint(0, endpoint_trim_dim + 1)	        randomized_seqs.append(seq[trim_length:-trim_length])	    return randomized_seqs		def make_single_cluster_data(master_sequence, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    inliners = [perturb(master_sequence, inliner_noise) for _ in range(n_inliners)]	    # outliers = [perturb(permute(master_sequence), outlier_noise) for _ in range(n_outliers)]	    outliers = [perturb(master_sequence, outlier_noise) for _ in range(n_outliers)]		    all_seqs = inliners + outliers 	        	    if endpoint_trim_dim is not None:	        all_seqs = randomize_length(all_seqs, endpoint_trim_dim)	    return all_seqs		def make_data(master_sequence, n_clusters, cluster_centres_noise, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    sequences, targets = [], []	    for i in range(n_clusters):	        cluster_centre = perturb(master_sequence, cluster_centres_noise)	        cluster_seqs = make_single_cluster_data(cluster_centre, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim)	        sequences.extend(cluster_seqs)	        targets.extend([i] * len(cluster_seqs))	    return sequences, targets	    	save_history()
2024-04-23-21-42-57 392 39 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 40 import random	    	def permute(seq):	    # Converting to list, permuting, and converting back to string	    seq_array = np.array(list(seq))	    return ''.join(np.random.permutation(seq_array))		def generate_master_sequence(alphabet_size, seq_length, start_char=70):	    return ''.join(chr(np.random.randint(start_char, start_char + alphabet_size)) for _ in range(seq_length))		def perturb(master_sequence, noise):	    seq_array = np.array(list(master_sequence))	    num_perturbations = int(len(seq_array) * noise)	    # Depending on the noise level swaping the places of random characters	    for _ in range(num_perturbations):	        i1, i2 = np.random.choice(len(seq_array), 2, replace=False)	        seq_array[i1], seq_array[i2] = seq_array[i2], seq_array[i1]	    return ''.join(seq_array)		def randomize_length(seqs, endpoint_trim_dim):	    randomized_seqs = []	    for seq in seqs:	        trim_length = np.random.randint(0, endpoint_trim_dim + 1)	        randomized_seqs.append(seq[trim_length:-trim_length])	    return randomized_seqs		def make_single_cluster_data(master_sequence, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    inliners = [perturb(master_sequence, inliner_noise) for _ in range(n_inliners)]	    # outliers = [perturb(permute(master_sequence), outlier_noise) for _ in range(n_outliers)]	    outliers = [perturb(master_sequence, outlier_noise) for _ in range(n_outliers)]		    all_seqs = inliners + outliers 	        	    if endpoint_trim_dim is not None:	        all_seqs = randomize_length(all_seqs, endpoint_trim_dim)	    	    print(all_seqs)	    return all_seqs		def make_data(master_sequence, n_clusters, cluster_centres_noise, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    sequences, targets = [], []	    for i in range(n_clusters):	        cluster_centre = perturb(master_sequence, cluster_centres_noise)	        cluster_seqs = make_single_cluster_data(cluster_centre, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim)	        sequences.extend(cluster_seqs)	        targets.extend([i] * len(cluster_seqs))	    return sequences, targets	    	save_history()
2024-04-23-21-42-57 392 41 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 42 import random	    	def permute(seq):	    # Converting to list, permuting, and converting back to string	    seq_array = np.array(list(seq))	    return ''.join(np.random.permutation(seq_array))		def generate_master_sequence(alphabet_size, seq_length, start_char=70):	    return ''.join(chr(np.random.randint(start_char, start_char + alphabet_size)) for _ in range(seq_length))		def perturb(master_sequence, noise):	    seq_array = np.array(list(master_sequence))	    num_perturbations = int(len(seq_array) * noise)	    # Depending on the noise level swaping the places of random characters	    for _ in range(num_perturbations):	        i1, i2 = np.random.choice(len(seq_array), 2, replace=False)	        seq_array[i1], seq_array[i2] = seq_array[i2], seq_array[i1]	    return ''.join(seq_array)		def randomize_length(seqs, endpoint_trim_dim):	    randomized_seqs = []	    for seq in seqs:	        trim_length = np.random.randint(0, endpoint_trim_dim + 1)	        randomized_seqs.append(seq[trim_length:-trim_length])	    return randomized_seqs		def make_single_cluster_data(master_sequence, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    inliners = [perturb(master_sequence, inliner_noise) for _ in range(n_inliners)]	    # outliers = [perturb(permute(master_sequence), outlier_noise) for _ in range(n_outliers)]	    outliers = [perturb(master_sequence, outlier_noise) for _ in range(n_outliers)]		    all_seqs = inliners + outliers 	        	    if endpoint_trim_dim is not None:	        all_seqs = randomize_length(all_seqs, endpoint_trim_dim)	    	    return all_seqs		def make_data(master_sequence, n_clusters, cluster_centres_noise, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim=None):	    sequences, targets = [], []	    for i in range(n_clusters):	        cluster_centre = perturb(master_sequence, cluster_centres_noise)	        cluster_seqs = make_single_cluster_data(cluster_centre, n_inliners, n_outliers, inliner_noise, outlier_noise, endpoint_trim_dim)	        sequences.extend(cluster_seqs)	        targets.extend([i] * len(cluster_seqs))	    return sequences, targets	    	save_history()
2024-04-23-21-42-57 392 43 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		print(len(seqs), seqs[0])		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 44 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	print(X)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 45 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	print(X.max())	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 46 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 47 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 48 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X, X_pca)	    print(X_pca.shape, X_pca.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 49 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 50 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 51 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 52 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 53 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		print(seqs.max())		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 54 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		print(max(seqs))		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 55 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]\	    print(vectorized_seqs.max())	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 56 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]\	    print(max(vectorized_seqs))	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 57 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]\	    print(max(vectorized_seqs))	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 58 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    print(max(vectorized_seqs))	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 59 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)		print(max(seqs))		X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 60 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 61 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        print(max(vector))	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    print(max(vectorized_seqs))	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 62 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 63 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	print(X)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 64 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    print(X_pca.shape, X_pca.max(), X.max())		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 65 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	print(X)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 66 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	max_elem = max(max(row) for row in array_2d)	print("Maximum element in the 2D array:", max_elem)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 67 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	max_elem = max(max(row) for row in X)	print("Maximum element in the 2D array:", max_elem)	pca_plot(X, y=targets)
2024-04-23-21-42-57 392 68 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    max_elem = max(max(row) for row in X_pca)	    print("Maximum element in the 2D array:", max_elem)		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 69 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    max_elem = max(max(row) for row in X_pca)	    print("Maximum element in the 2D array:", max_elem)		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 70 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 71 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)	flat_array = [elem for row in X for elem in row]		# Calculate the average	average = sum(flat_array) / len(flat_array)		print("Average of the elements in the 2D array:", average)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 72 def histogram_vectorizer(seqs):	    def convert_to_int_sequence(seq):	        return [ord(char) for char in seq]		    def single_histogram_vectorizer(seq):	        int_seq = convert_to_int_sequence(seq)	        vector = [int_seq.count(i) for i in range(256)] # counting eveery ASCII character	        return vector		    vectorized_seqs = [single_histogram_vectorizer(seq) for seq in seqs]	    return np.array(vectorized_seqs)	        		from sklearn.decomposition import PCA		def pca_plot(X, y=None):	    # Performing PCA and reducing the dimentions to 2D	    pca = PCA(n_components=2)	    X_pca = pca.fit_transform(X)	    max_elem = max(max(row) for row in X_pca)	    print("Maximum element in the 2D array:", max_elem)	    flat_array = [elem for row in X_pca for elem in row]	    average = sum(flat_array) / len(flat_array)	    print("Average of the elements in the 2D array:", average)		    plt.figure(figsize=(8, 6))	    if y is None:	        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.8, cmap='viridis')	    else:	        colors_for_labels = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'w']	        unique_labels = np.unique(y)	        for label in unique_labels:	            plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=colors_for_labels[label], label=label, s=50, alpha=0.8)	    plt.xlabel('PCA1')	    plt.ylabel('PCA2')	    plt.show()		save_history()
2024-04-23-21-42-57 392 73 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 74 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.099, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 75 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.00, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 76 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
2024-04-23-21-42-57 392 77 master_sequence = generate_master_sequence(alphabet_size=4, seq_length=150, start_char=68)	n_instances = 200	n_outliers = n_instances//9	n_inliners = n_instances - n_outliers	seqs, targets = make_data(	    master_sequence, 	    n_clusters=2,	    cluster_centres_noise=.5,	    n_inliners=n_inliners, 	    n_outliers=0, 	    inliner_noise=.15, 	    outlier_noise=.99, 	    endpoint_trim_dim=15)			X = histogram_vectorizer(seqs)		pca_plot(X, y=targets)
