2024-03-11-22-55-33 384 1 get_ipython().run_line_magic('matplotlib', 'inline')	import matplotlib.pyplot as plt	from mpl_toolkits.mplot3d import Axes3D	import numpy as np	import scipy as sp	from submission_utils import save_history, check_and_prepare_for_submission	# import warnings filter	from warnings import simplefilter	# ignore all future warnings	simplefilter(action='ignore', category=FutureWarning)
2024-03-11-22-55-33 384 2 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 3 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets)	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets)	    	    plt.show()		save_history()
2024-03-11-22-55-33 384 4 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 5 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets)	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets)	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3') 	    plt.show()		save_history()
2024-03-11-22-55-33 384 6 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 7 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='magma')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='magma')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3') 	    plt.show()		save_history()
2024-03-11-22-55-33 384 8 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 9 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3') 	    plt.show()		save_history()
2024-03-11-22-55-33 384 10 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 11 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    ax.legend(loc='best') 	    plt.show()		save_history()
2024-03-11-22-55-33 384 12 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 13 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    ax.legend(loc='best') 	    plt.colorbar(label='Target')	    plt.show()		save_history()
2024-03-11-22-55-33 384 14 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 15 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))	    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 16 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 17 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    # informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    informative_inliners = np.random.multivariate_normal(np.ones(n_informative_features), np.eye(n_informative_features) * std**2	, size=n_inliners)	    	    # non_informative_inliners = np.random.multivariate_normal(np.zeros(n_non_informative_features), np.eye(n_non_informative_features) * std**2  , size=n_inliners)	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    informative_outliers = np.random.multivariate_normal(-np.ones(n_informative_features), np.eye(n_informative_features) * outliers_std**2, size=(n_outliers,))		    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.multivariate_normal(-np.ones(n_informative_features), np.eye(n_informative_features) * std**2, size=(n_informative_features,))	    # informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 18 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 19 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    # informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    informative_inliners = np.random.multivariate_normal(np.ones(n_informative_features), np.eye(n_informative_features) * std**2	, size=n_inliners)	    	    # non_informative_inliners = np.random.multivariate_normal(np.zeros(n_non_informative_features), np.eye(n_non_informative_features) * std**2  , size=n_inliners)	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    informative_outliers = np.random.multivariate_normal(-np.ones(n_informative_features), np.eye(n_informative_features) * outliers_std**2, size=(n_outliers,))		    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.multivariate_normal(-np.ones(n_informative_features), np.eye(n_informative_features) * std**2, size=(n_instances,))	    # informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 20 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 21 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    # print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 22 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 23 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 24 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(np.ones(n_inliners), std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-np.ones(n_outliers), outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    # print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 25 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 26 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    # print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 27 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 28 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    # print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 29 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 30 def make_positive_data(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    # Calculation of the number of outliers and inliners	    n_inliners = int((1 - fraction_of_outliers) * n_instances)	    n_outliers = int(fraction_of_outliers * n_instances)	    # print(f'Normal: {n_inliners}, outliers {n_outliers}')		    # Generate inliners	    informative_inliners = np.random.normal(1, std, (n_inliners, n_informative_features))	    non_informative_inliners = np.random.normal(0, std, (n_inliners, n_non_informative_features))		    # print(f'Informative inliners: {informative_inliners.shape}, non-informative inliners: {non_informative_inliners.shape}')	    # print(f'{informative_inliners[:5]}')	    # print(f'{non_informative_inliners[:5]}')		    # Generate outliers	    informative_outliers = np.random.normal(-1, outliers_std, (n_outliers, n_informative_features))	    non_informative_outliers = np.random.normal(0, outliers_std, (n_outliers, n_non_informative_features))	    # print(f'Informative outliers: {informative_outliers.shape}, non-informative outliers: {non_informative_outliers.shape}')	    # print(f'{informative_outliers[:5]}')	    # print(f'{non_informative_outliers[:5]}')		    # Join all generated data	    all_positive_data = np.concatenate((np.hstack((informative_inliners, non_informative_inliners)),	                                    np.hstack((informative_outliers, non_informative_outliers))))	    # print(f'All data: {all_positive_data.shape}')	    # print(f'All data: {type(all_positive_data)}')	    return all_positive_data		def make_negative_data(n_instances, n_informative_features, n_non_informative_features, std):	    informative = np.random.normal(-1, std, (n_instances, n_informative_features))	    non_informative = np.random.normal(0, std, (n_instances, n_non_informative_features))	    # print(f'Informative: {informative.shape}, non-informative: {non_informative.shape}')	    # print(f'{informative[:5]}')	    # print(f'{non_informative[:5]}')		    all_negative_data = np.hstack((informative, non_informative))	    # print(all_negative_data.shape)	    # print(all_negative_data[:5])	    # print(type(all_negative_data))	    return all_negative_data	    		def make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std):	    amount_of_positives = n_instances // 2	    amount_of_negatives = n_instances - amount_of_positives	    positive_data = make_positive_data(n_instances=amount_of_positives, 	                                       fraction_of_outliers=fraction_of_outliers, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std, 	                                       outliers_std=outliers_std)	    negative_data = make_negative_data(n_instances=amount_of_negatives, 	                                       n_informative_features=n_informative_features, 	                                       n_non_informative_features=n_non_informative_features, 	                                       std=std)	    	    data_mtx = np.concatenate((positive_data, negative_data))	    targets = np.concatenate((np.ones(amount_of_positives), np.zeros(amount_of_negatives)))	    # print(f'Final data: {data_mtx.shape}')	    # print(f'Targets: {targets.shape}')	    # print(data_mtx[:5]), print(targets[:5])	    return data_mtx, targets		def plot2d(data_mtx, targets=None, title='', size=8):	    plt.figure(figsize=(size, size))	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1])	    else: 	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], c=targets, cmap='viridis')	    plt.title(title)	    plt.show()  		def plot3d(data_mtx, targets=None, title='', size=8):	    fig = plt.figure(figsize=(size, size))	    ax = fig.add_subplot(111, projection='3d')	    if targets is None:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2])	    else:	        plt.scatter(data_mtx[:, 0], data_mtx[:, 1], data_mtx[:, 2], c=targets, cmap='viridis')	    plt.title(title)	    ax.set_xlabel('Column 1')	    ax.set_ylabel('Column 2')	    ax.set_zlabel('Column 3')	    plt.show()		save_history()
2024-03-11-22-55-33 384 31 # Just run the following code, do not modify it	n_instances = 1000	fraction_of_outliers = 0.3	n_informative_features = 1	n_non_informative_features = 2	std = .5	outliers_std = 5		data_mtx, targets = make_dataset(n_instances, fraction_of_outliers, n_informative_features, n_non_informative_features, std, outliers_std)		plot2d(data_mtx, targets, title='Data 2D', size=8)	plot3d(data_mtx, targets, title='Data 3D', size=8)
2024-03-11-22-55-33 384 32 def rebalance(X, y):	    distinct_classes = np.unique(y)	    print(f'Number of distinct classes: {len(distinct_classes)}')		def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 33 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')		def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 34 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')		    max_amount_of_class = np.max(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 35 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.min(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 36 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        print(f'Label: {label}')	        label_indices = np.where(y == label)[0]	        print(f'Label indices: {label_indices}')	        		def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 37 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        print(f'Label: {label}')	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # YOUR CODE HERE	    raise NotImplementedError()		rebalance(data_mtx, targets)		save_history()
2024-03-11-22-55-33 384 38 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        print(f'Label: {label}')	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y, return_counts=True)			save_history()
2024-03-11-22-55-33 384 39 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        print(f'Label: {label}')	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    class_test_size = class_count * test_size	    print(f'Class test size: {class_test_size}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 40 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    class_test_size = class_count * test_size	    print(f'Class test size: {class_test_size}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 41 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    class_test_size = int(class_count * test_size)	    print(f'Class test size: {class_test_size}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 42 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    class_test_size = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_size}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 43 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 44 def rebalance(X, y):	    values, class_count = np.unique(y, return_counts=True)	    print(f'Number of distinct classes: {len(class_count)}, {values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 45 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}'		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 46 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 47 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.append(x_rb[train_indices])	        X_test.append(x_rb[test_indices])	        y_train.append(y_rb[train_indices])	        y_test.append(y_rb[test_indices])	    	    print(type(X_train), type(X_test), type(y_train), type(y_test))	    return X_train, X_test, y_train, y_test		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 48 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(X_rebalanced[train_indices])	        X_test.extend(X_rebalanced[test_indices])	        y_train.extend(y_rebalanced[train_indices])	        y_test.extend(y_rebalanced[test_indices])	    	    print(type(X_train), type(X_test), type(y_train), type(y_test))	    return X_train, X_test, y_train, y_test		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 49 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    print(type(X_train), type(X_test), type(y_train), type(y_test))	    return X_train, X_test, y_train, y_test		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 50 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 51 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 52 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 53 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-03-11-22-55-33 384 54 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        np.random.shuffle(class_indices)	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 55 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-03-11-22-55-33 384 56 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-03-11-22-55-33 384 57 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        np.random.shuffle(class_indices)	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 58 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        np.random.shuffle(class_indices)	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    # print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 59 def rebalance(X, y):	    unique_values, class_count = np.unique(y, return_counts=True)	    # print(f'Number of distinct classes: {len(class_count)}, {unique_values}')	    max_amount_of_class = np.max(class_count)	    # print(f'Maximum amount of class: {max_amount_of_class}')		    x_new, y_new = [], []		    for label in unique_values:	        label_indices = np.where(y == label)[0]	        oversampled_version = np.random.choice(label_indices, max_amount_of_class, replace=True)		        x_new.extend(X[oversampled_version])        	        y_new.extend(y[oversampled_version])		    # print(f'New data: {len(x_new)}, {len(y_new)}')	    return np.array(x_new), np.array(y_new)        			def rebalanced_stratified_split(X_orig, y_orig, test_size=0.2, random_state=None):	    # Empty train and test sets	    X_train, X_test, y_train, y_test = [], [], [], []		    # Rebalance function implementation	    x_rb, y_rb = rebalance(X=X_orig, y=y_orig)	    # Initialization of the random state	    np.random.seed(random_state)	    unique_values, class_count = np.unique(y_rb, return_counts=True)	    # print(class_count)	    class_test_sizes = np.round(class_count * test_size).astype(int)	    # print(f'Class test size: {class_test_sizes}')		    for class_label, test_size in zip(unique_values, class_test_sizes):	        class_indices = np.where(y_rb == class_label)[0]	        np.random.shuffle(class_indices)	        	        test_indices = class_indices[:test_size]	        train_indices = class_indices[test_size:]	        # print(f'Class: {class_label}, test: {len(test_indices)}, train: {len(train_indices)}')		        X_train.extend(x_rb[train_indices])	        X_test.extend(x_rb[test_indices])	        y_train.extend(y_rb[train_indices])	        y_test.extend(y_rb[test_indices])	    	    # print(X_train[:15], X_test[:15], y_train[:15], y_test[:15])	    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)		rebalanced_stratified_split(data_mtx, targets, test_size=0.2, random_state=42)		save_history()
2024-03-11-22-55-33 384 60 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    # YOUR CODE HERE	    raise NotImplementedError()		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    # YOUR CODE HERE	    raise NotImplementedError()		print(accuracy_score(np.array([1, 1, 0, 0]), np.array([1, 1, 1, 0]))) #Test		save_history()
2024-03-11-22-55-33 384 61 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    # YOUR CODE HERE	    raise NotImplementedError()		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        print(f'Accuracy for repetition {rep}: {acc}')			predictive_performance_estimate(classifier, data_mtx, targets, test_size=0.2, n_rep=3)	save_history()
2024-03-11-22-55-33 384 62 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    # YOUR CODE HERE	    raise NotImplementedError()		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    acc_scores = []	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        print(f'Accuracy for repetition {rep}: {acc*100}%')	        acc_scores.append(acc)	    return np.mean(acc_scores), np.std(acc_scores)		save_history()
2024-03-11-22-55-33 384 63 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    unique_values = np.unique(y_true)	    num_of_values = len(unique_values)	    conf_matrix = np.zeros((num_of_values, num_of_values)) 	    print(conf_matrix)		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    acc_scores = []	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        # print(f'Accuracy for repetition {rep}: {acc*100}%')	        acc_scores.append(acc)	    return np.mean(acc_scores), np.std(acc_scores)			confusion_matrix(targets, targets)	save_history()
2024-03-11-22-55-33 384 64 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    unique_values = np.unique(y_true)	    num_of_values = len(unique_values)	    conf_matrix = np.zeros((num_of_values, num_of_values)) 	    print(conf_matrix)		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    acc_scores = []	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        # print(f'Accuracy for repetition {rep}: {acc*100}%')	        acc_scores.append(acc)	    return np.mean(acc_scores), np.std(acc_scores)			conf_matrix = confusion_matrix(np.array([0, 1, 1, 0, 2, 2, 0]), np.array([0, 1, 0, 0, 2, 1, 2]))	save_history()
2024-03-11-22-55-33 384 65 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    unique_values = np.unique(y_true)	    num_of_values = len(unique_values)	    conf_matrix = np.zeros((num_of_values, num_of_values)) 	    print(conf_matrix)		    for true_label, pred_label in zip(y_true, y_pred):	        true_index = np.where(unique_values == true_label)[0][0]	        pred_index = np.where(unique_values == pred_label)[0][0]	        conf_matrix[true_index, pred_index] += 1		    return conf_matrix.astype(int)	    		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    acc_scores = []	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        print(f'Accuracy for repetition {rep}: {acc*100}%')	        acc_scores.append(acc)	    return np.mean(acc_scores), np.std(acc_scores)		predictive_performance_estimate(np.array([1, 1, 1, 0, 0, 0]), np.array([1, 1, 1, 0, 0, 0]), 0.2, 3)		save_history()
2024-03-11-22-55-33 384 66 def accuracy_score(y_true, y_pred):	    num_of_correct_preds = np.sum(y_true == y_pred)	    total_preds = len(y_true)	    return num_of_correct_preds / total_preds		def confusion_matrix(y_true, y_pred):	    unique_values = np.unique(y_true)	    num_of_values = len(unique_values)	    conf_matrix = np.zeros((num_of_values, num_of_values)) 	    print(conf_matrix)		    for true_label, pred_label in zip(y_true, y_pred):	        true_index = np.where(unique_values == true_label)[0][0]	        pred_index = np.where(unique_values == pred_label)[0][0]	        conf_matrix[true_index, pred_index] += 1		    return conf_matrix.astype(int)	    		def predictive_performance_estimate(classifier, data_mtx, targets, test_size, n_rep=3):	    acc_scores = []	    for rep in range(n_rep):	        X_train, X_test, y_train, y_test = rebalanced_stratified_split(data_mtx, targets, test_size=test_size, random_state=rep)	        classifier.fit(X_train, y_train)	        y_pred = classifier.predict(X_test)	        acc = accuracy_score(y_test, y_pred)	        print(f'Accuracy for repetition {rep}: {acc*100}%')	        acc_scores.append(acc)	    return np.mean(acc_scores), np.std(acc_scores)		predictive_performance_estimate(np.array([1, 1, 1, 0, 0, 0]), np.array([1, 1, 1, 0, 0, 0]), 0.2, 3)		accuracy_score(np.array([1, 1, 1, 0, 0, 0]), np.array([1, 1, 1, 0, 0, 0]))	save_history()
